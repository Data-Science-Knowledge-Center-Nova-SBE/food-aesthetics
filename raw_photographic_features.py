# -*- coding: utf-8 -*-
"""photographic_features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12jar8doRqy8Xz4DyvHWNjRGJ5OMbT7Hy
"""

# import libraries
import cv2 as cv
from skimage import io
#from google.colab.patches import cv2_imshow # for image display
import numpy as np
import matplotlib.pyplot as plt
import pywt
import pandas as pd
from skimage.exposure import rescale_intensity
from skimage.segmentation import slic
from skimage.util import img_as_float
from skimage.measure import label, regionprops, regionprops_table
from scipy.spatial import distance
from PIL import Image


# methods to read pic
def image2hsv(pic_id):
    """"
    input: image ID
    output: HSV matrix w/ shape H x W x C, C = 3 for H, S, and V respectively
    """
    img = np.array(Image.open(pic_id))
    return cv.cvtColor(img, cv.COLOR_BGR2HSV)

def image2rgb(pic_id):
    """
    input: image ID
    output: RGB matrix w/ shape H x W x C, C = 3 for Red , Green and Blue respectively
    """
    return cv.cvtColor(io.imread(pic_id), cv.COLOR_BGR2RGB)

"""# COLOR FEATURE EXTRACTORS """
def brightness(image):
    """
    Cross Pixel Average of Value (2) dimension

    input: H x W x C HSV image
    output: brightness value range [0, 255]
    """
    return image[:,:,2].mean()

def saturation(image):
    """
    Cross Pixel Average of Saturaion (1) dimension

    input: H x W x C HSV image
    output: saturation value range [0, 255]
    """
    return image[:, :, 1].mean()

def contrast(image):
    """
    Cross Pixel Standard Deviation of Value (2) dimension

    input: H x W x C HSV image
    output: contrast value range [0, n]
    """
    return image[:, :, 2].std()

def clarity(image, thresold = 0.7, scaler = 255):
    """
    Proportion of Normalized Value (2) Pixels that Exceed the Thresold (0.7)

    input: H x W x C HSV image
    output: clarity value range [0, 1]
    """
    h, w, c = image.shape
    return np.sum(image[:,:,2] / scaler > thresold) / (h * w)

def warm(image):
    """"
    Proporion of Warm Hue (<30, >110) Pixels

    input: H x W x C HSV image
    output: warm value range [0, 1]
    """
    h, w, c = image.shape
    return np.sum((image[:, :, 0] < 30) | (image[:, :, 0] > 110)) / (h * w)

def colourfulness(image):
    """
    Follow Hasler and Suesstrunk (2003) to compute colourfoulness

    input: H x W x C RGB image
    output: colourfoluness score
    """
    R, G, B = image[:,:,0], image[:,:,1], image[:,:,2]
    rg = R - G
    yb = 0.5 * (R+G) - B
    sigma = np.sqrt(np.square(np.std(rg)) + np.square(np.std(yb)))
    mu = np.sqrt(np.square(np.mean(rg)) + np.square(np.mean(yb)))
    c = sigma + 0.3 * mu # colourfoulness
    return c


"""# Figure-Ground Relationships """
def grabcut(image, n_iter = 10):
    """"
    Performs GrabCut according to:
    https://docs.opencv.org/master/d8/d83/tutorial_py_grabcut.html
    output: a binary FOREGROUND mask of dimension H x W in which 1 = foreground, 0 = background
    """
    mask = np.zeros(image.shape[:2],np.uint8) # mask of shape of the image
    bgdModel = np.zeros((1,65),np.float64) # take as given by docs
    fgdModel = np.zeros((1,65),np.float64) # take as given by docs

    # see description later
    rect = (1,1,image.shape[0]-1, image.shape[1]-1)
    # starts at coordinates 0,0 and init a rectangle of size -1 w/ respect both to
    # width and to height
    cv.grabCut(image ,mask,rect, bgdModel, fgdModel, n_iter, cv.GC_INIT_WITH_RECT)
    return np.where((mask==2)|(mask==0),0,1).astype('uint8')

# size difference
def size_difference(mask):
    """
    Computes Size Difference between Foreground and Background

    input: mask matrix, 1s for foreground, 0s for background
    output: normalized size difference
    """
    h, w = mask.shape
    counts = np.unique(mask, return_counts=True)

    #{counts[0][0]:counts[1][0], counts[0][1]:counts[1][1]}
    # 1 foreground, 0 background
    n_for = counts[1][1]
    n_back = counts[1][0]
    return (n_for - n_back) / (h * w)

# color difference
def color_difference(image, mask):
    """
    Computes the Euclidean distance between each RGB channel of Foreground compared to Background

    inputs: image and foreground mask of equal dimensions
    output: Euclidean distance
    """

    # compute front
    front = image*mask[:,:,np.newaxis]

    # compute back
    back_mask = np.where(mask == 1, 0, 1) # jsut flip the original mask to detect the front
    back = image * back_mask[:,:, np.newaxis]

    # R, G, B
    r_front, g_front, b_front = front[:,:,0].flatten(), front[:,:,1].flatten(), front[:,:,2].flatten()
    r_back, g_back, b_back = back[:,:,0].flatten(), back[:,:,1].flatten(), back[:,:,2].flatten()

    # create vectors (front and back) containing the average value for each channel - exclude the 0s from both front and back
    avg_front = np.array([r_front[r_front != 0].mean(), g_front[g_front != 0].mean(), b_front[b_front != 0].mean()])
    avg_back = np.array([r_back[r_back != 0].mean(), g_back[g_back != 0].mean(), b_back[b_back != 0].mean()])

    return np.linalg.norm(avg_front - avg_back)


def texture_difference(image, mask):
    """
    Let's use this
    """

    # compute front
    front = image*mask[:,:,np.newaxis]
    front_flattened = front.flatten()
    front_shape = front_flattened[front_flattened != 0].shape[0] # number of pixels of the front
    #print(front_shape)

    # compute back
    back_mask = np.where(mask == 1, 0, 1) # jsut flip the original mask to detect the front
    back = image * back_mask[:,:, np.newaxis].astype(np.uint8)
    back_flattened = back.flatten()
    back_shape = back_flattened[back_flattened != 0].shape[0] # number of pixels of the back
    #print(back_shape)

    # edges front and back
    edges_front = cv.Canny(front, front.shape[0], front.shape[1])
    edges_back = cv.Canny(back, back.shape[0], back.shape[1])

    return np.abs((np.sum(edges_front) / front_shape) - (np.sum(edges_back) / back_shape)) # fix this



"""# COMPOSITION"""

# DIAGONAL DOMINANCE
def segment_colorfulness(image, mask):
    # split the image into its respective RGB components, then mask
    # each of the individual RGB channels so we can compute
    # statistics only for the masked region
    (B, G, R) = cv.split(image.astype("float"))
    R = np.ma.masked_array(R, mask=mask)
    G = np.ma.masked_array(G, mask=mask)
    B = np.ma.masked_array(B, mask=mask)
    # compute rg = R - G
    rg = np.absolute(R - G)
    # compute yb = 0.5 * (R + G) - B
    yb = np.absolute(0.5 * (R + G) - B)
    # compute the mean and standard deviation of both `rg` and `yb`,
    # then combine them
    stdRoot = np.sqrt((rg.std() ** 2) + (yb.std() ** 2))
    meanRoot = np.sqrt((rg.mean() ** 2) + (yb.mean() ** 2))
    # derive the "colorfulness" metric and return it
    return stdRoot + (0.3 * meanRoot)


def calculate_saliency(image):
    saliency = cv.saliency.StaticSaliencyFineGrained_create()
    (success, saliencyMap) = saliency.computeSaliency(image)

    saliencyMap_uint8 = (255 * saliencyMap).astype('uint8')
    threshMap = cv.threshold(saliencyMap_uint8, 0, 255,cv.THRESH_BINARY + cv.THRESH_OTSU)[1]

    return saliencyMap

def segment_image(image):
    # create the image segments based on colorfulness
    # load the image and apply SLIC superpixel segmentation to it via
    # scikit-image
    segments = slic(img_as_float(image), n_segments=10, slic_zero=True)

    return segments

def calculate_saliency_region(image):

    segments = segment_image(image)
    saliencyMap = calculate_saliency(image)
    # Calculate the average saliency for all segments
    saliency_region_id = []

    for v in np.unique(segments):
        # construct a region-of-interest to obtain the saliency
        # score for each segment
        roi = np.ones(image.shape[:2])
        roi[segments == v] = 0

        # calculate saliency score for each segment
        saliency_segment_score = np.mean(np.ma.masked_array(saliencyMap, mask=roi.astype(bool)))
        saliency_segment = np.ma.masked_array(saliencyMap, mask=roi.astype(bool))
        saliency_segment = rescale_intensity(saliency_segment, out_range=(0, 255)).astype("uint8")

        #print(v, np.ma.masked_array(saliencyMap, mask=roi.astype(bool)).count(), saliency_segment_score)
        saliency_region_id.append(saliency_segment_score)

    return saliency_region_id

def get_saliency_region_centroid(image):
    # calculate the centroid of saliency region in segment
    # with mean saliency score
    segments = segment_image(image)
    max_saliency_region_id = calculate_saliency_region(image)
    vis = np.zeros(image.shape[:2], dtype="float")

    salient_region_centroid_list = []

    for v in np.unique(segments):
        vis = np.zeros(image.shape[:2], dtype="float")
        #if v == max_saliency_region_id:
        # construct a mask for the segment so we can compute image
        # statistics for *only* the masked region
        mask = np.ones(image.shape[:2])
        mask[segments == v] = 0
        # compute the superpixel colorfulness, then update the
        # visualization array
        C = segment_colorfulness(image, mask)
        vis[segments == v] = C

        # scale the visualization image from an unrestricted floating point
        # to unsigned 8-bit integer array so we can use it with OpenCV and
        # display it to our screen
        vis = rescale_intensity(vis, out_range=(0, 255)).astype("uint8")

        # calculate region centroid
        salient_region =  np.zeros(image.shape[:2])
        salient_region[vis==255] = 1
        salient_region_label = label(salient_region)
        region_properties = regionprops(salient_region_label)
        salient_region = rescale_intensity(salient_region, out_range=(0, 255)).astype("uint8")

        props = regionprops_table(salient_region_label, properties=['centroid'])
        salient_region_centroid = pd.DataFrame(props).values

        salient_region_centroid_list.append(salient_region_centroid)

    return salient_region_centroid_list

def get_diagonal_dominance(image):
    """
    Get Diagonal Dominance of an Image.
    input: image RGB in format H x W x C, C=3
    output:
    """
    salient_region_centroid_list = get_saliency_region_centroid(image)
    maximum_salient_region = np.argmax(calculate_saliency_region(image))
    salient_region_centroid = salient_region_centroid_list[maximum_salient_region]

    # Calculate Manhattan distance between centroid of salient region to two diagonals
    dim = np.min(image.shape[:2])
    diagonal_distance1 = []
    diagonal1_row, diagonal1_col = np.diag_indices(dim)

    for idx in range(dim):
        diagonal_pos1 = np.array([diagonal1_row[idx], diagonal1_col[idx]])
        diagonal_distance1.append(distance.cityblock(salient_region_centroid, diagonal_pos1))

        diagonal_distance2 = []
        diagonal2_row, diagonal2_col = np.diag_indices(dim)
        for idx in range(dim):
            diagonal_pos2 = np.array([dim-1-diagonal2_row[idx], diagonal2_col[idx]])
            diagonal_distance2.append(distance.cityblock(salient_region_centroid, diagonal_pos2))

            # diagonal dominance as the negative of the minimum of two distances
            diagonal_dominance = -np.min([np.min(diagonal_distance1), np.min(diagonal_distance2)])
    return diagonal_dominance



# RULE OF THIRDS
# Generate gridlines to separate the image into 9 parts
def generate_gridlines(dim):
    # Generate 2 horizontal lines
    point_1 = np.array([0, dim//3])
    point_2 = np.array([dim-1, dim//3])
    point_3 = np.array([0, dim//3*2])
    point_4 = np.array([dim-1, dim//3*2])

    # Generate 2 vertical lines
    point_5 = np.array([dim//3, 0])
    point_6 = np.array([dim//3, dim-1])
    point_7 = np.array([dim//3*2, 0])
    point_8 = np.array([dim//3*2, dim-1])

    horizontal_lines = [[point_1, point_2], [point_3, point_4]]
    vertical_lines = [[point_5, point_6], [point_7, point_8]]

    return horizontal_lines, vertical_lines

# Calculate intersection points between a horizontal line and a vertical line
def line_intersection(horizontal_line, vertical_line):
    #line ab
    point_a = horizontal_line[0]
    point_b = horizontal_line[1]
    a1 = point_b[1] - point_a[1]
    b1 = point_a[0] - point_b[0]
    c1 = a1 * point_a[0] + b1 * point_a[1]

    #line cd
    point_c = vertical_line[0]
    point_d = vertical_line[1]
    a2 = point_d[1] - point_c[1]
    b2 = point_c[0] - point_d[0]
    c2 = a2 * point_c[0] + b2 * point_c[1]

    determinant = a1 * b2 - a2 * b1

    x = (b2 * c1 - b1 * c2)/determinant
    y = (a1 * c2 - a2 * c1)/determinant

    return np.array([x, y])

"""
def get_rule_of_thirds(image):

    dim = np.min(image.shape[:2])
    salient_region_centroid_list = get_saliency_region_centroid(image)
    maximum_salient_region = calculate_saliency_region(image)
    salient_region_centroid = salient_region_centroid_list[maximum_salient_region]

    horizontal_lines, vertical_lines = generate_gridlines(dim)
    rule_of_thirds_distance = []

    for horizontal_idx in range(2):
        for vertical_idx in range(2):
            gridline_intersection = line_intersection(horizontal_lines[horizontal_idx], vertical_lines[vertical_idx])
            rule_of_thirds_distance.append(distance.cityblock(salient_region_centroid, gridline_intersection))

            rule_of_thirds_score = -np.min(rule_of_thirds_distance)

    return rule_of_thirds_score
"""

def get_rule_of_thirds(image):
  dim = np.min(image.shape[:2])
  salient_region_centroid_list = get_saliency_region_centroid(image)
  maximum_salient_region = np.argmax(calculate_saliency_region(image))
  salient_region_centroid = salient_region_centroid_list[maximum_salient_region]

  horizontal_lines, vertical_lines = generate_gridlines(dim)
  rule_of_thirds_distance = []

  for horizontal_idx in range(2):
    for vertical_idx in range(2):
      gridline_intersection = line_intersection(horizontal_lines[horizontal_idx], vertical_lines[vertical_idx])
      rule_of_thirds_distance.append(distance.cityblock(salient_region_centroid, gridline_intersection))

  rule_of_thirds_score = -np.min(rule_of_thirds_distance)

  return rule_of_thirds_score

"""# Calculate visual balance"""

def get_physical_visual_balance(image):

    # Calculate the weighted saliency score across segments

    horizontal_salient_list = []
    vertical_salient_list = []

    dim = np.min(image.shape[:2])
    image = cv.resize(image, (dim, dim))


    saliency_region_id =  calculate_saliency_region(image)
    #print(saliency_region_id)

    salient_region_centroid_list = get_saliency_region_centroid(image)
    #print(salient_region_centroid_list)

    for idx, value in enumerate(saliency_region_id):
        horizontal_salient_list.append(salient_region_centroid_list[idx][0][0])
        vertical_salient_list.append(salient_region_centroid_list[idx][0][1])

    horizontal_mass, vertcial_mass = np.average(horizontal_salient_list , weights=saliency_region_id), np.average(vertical_salient_list , weights=saliency_region_id)
    horizontal_physical_visual_balance, vertcial_physical_visual_balance =  -np.abs(horizontal_mass - image.shape[0]/2), -np.abs(vertcial_mass - image.shape[1]/2)

    return horizontal_physical_visual_balance, vertcial_physical_visual_balance


"""# Calculate the color visual balance"""

def get_color_visual_balance(image):
    # get color visual balance as average Euclidean distance between pairs of
    # pixels in symmetric position across horizontal and vertical center
    dim = np.min(image.shape[:2])
    image = cv.resize(image, (dim, dim))
    half = dim//2
    upper_half = image[:half]
    lower_half_flipped = np.flip(image[half:], axis=0)

    vertical_color_visual_balance =  -np.abs(np.linalg.norm(upper_half-lower_half_flipped))/upper_half.size

    left_half = image[:, :half]
    right_half_flipped = np.flip(image[:, half:], axis=0)
    horizontal_color_visual_balance =  -np.abs(np.linalg.norm(left_half-right_half_flipped))/left_half.size
    return vertical_color_visual_balance, horizontal_color_visual_balance


# to rescale pictures
def rescale_down(img, scale_percent = 30):
    #scale_percent: percent of original size
    width = int(img.shape[1] * scale_percent / 100)
    height = int(img.shape[0] * scale_percent / 100)
    dim = (width, height)

    # resize image
    resized = cv.resize(img, dim, interpolation = cv.INTER_AREA)
    return resized


if __name__ == '__main__':
    img = "./test-images/image1.jpeg"
    test = image2hsv(img)
    print(test.shape)
